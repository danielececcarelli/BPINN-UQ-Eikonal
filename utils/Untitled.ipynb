{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, activations\n",
    "\n",
    "\n",
    "class Net:\n",
    "    \"\"\"Define the base neural network model: Fully Connected Network\n",
    "\n",
    "    Model defined with tf.keras.Sequential() module\n",
    "\n",
    "    Args:\n",
    "    - n_input (2 or 3)\n",
    "    - n_hidden_layer\n",
    "    - n_hidden_neuron\n",
    "    - n_output\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden_layer, n_hidden_neuron, n_output):\n",
    "        \"\"\"Constructor: build our neural network architecture\"\"\"\n",
    "        self.features = tf.keras.Sequential()   # use sequential to build the model\n",
    "\n",
    "        self.features.add(tf.keras.Input(shape=(n_input,)))    #input\n",
    "\n",
    "        for i in range(n_hidden_layer): #for loop to add the n_hidden_layers (with n_hidden_neuron each)\n",
    "            self.features.add(layers.Dense(n_hidden_neuron, activation=\"swish\", kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "\n",
    "        self.features.add(layers.Dense(n_output,  kernel_initializer='glorot_uniform', bias_initializer='zeros'))   #output\n",
    "\n",
    "\n",
    "    # return the number of parameters in the network    # TODO: change -> mettere un metodo più furbo...\n",
    "    def num_parameters(self):\n",
    "        \"\"\"return the number of parameters in the network\"\"\"\n",
    "        tot = 0\n",
    "        for layer in self.features.layers:\n",
    "            (dim1,dim2) = layer.get_weights()[0].shape\n",
    "            (dim3,) = layer.get_weights()[1].shape\n",
    "            tot+= dim1*dim2\n",
    "            tot+= dim3\n",
    "        return tot\n",
    "\n",
    "    # return a list of all dimension of W and b in each layer\n",
    "    def get_dimensions(self):\n",
    "        \"\"\"return a list of all dimension of W and b in each layer\"\"\"\n",
    "        architecture = []\n",
    "        for layer in self.features.layers:\n",
    "            (dim1,dim2) = layer.get_weights()[0].shape # dims of matrix W\n",
    "            (dim3,) = layer.get_weights()[1].shape  # dim of vector bias\n",
    "            architecture.append((dim1,dim2,dim3))\n",
    "        return architecture\n",
    "\n",
    "    #decorator to speed up, alternatively directly call to Net.features(input)\n",
    "    @tf.function\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass of the input x\"\"\"\n",
    "        return self.features(x)\n",
    "\n",
    "    # return a list of all the matrix W and bias vectors in the network (e.g. shape=(8,))\n",
    "    def get_parameters(self):\n",
    "        return self.features.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "class BayesNN:\n",
    "    \"\"\"\n",
    "    Define PINN-Bayesian network as a collection of NNs with physics-constraint\n",
    "\n",
    "    Args:\n",
    "    - num_neural_networks\n",
    "    \"\"\"\n",
    "    def __init__(self, num_neural_networks, pde_noise, nFeature, nLayer, nNeuron, nOutput_vel, param2loss):\n",
    "        \"\"\"\"Constructor\"\"\"\n",
    "\n",
    "        self.num_neural_networks = num_neural_networks # number of Neural Networks used\n",
    "\n",
    "        # w_i ~ StudentT(w_i | mu=0, lambda=shape/rate, nu=2*shape)\n",
    "        # for efficiency, represent StudentT params using Gamma params\n",
    "        # Nick shape  = 0.5, rate = 10\n",
    "        self.w_prior_shape = 1.     # TODO: perchè poi inizializziamo w con una normale inizial. di una Neural Network?\n",
    "        self.w_prior_rate = 0.05    # cioè con Normal(0, sigma) per matrice W e zero per Bias?\n",
    "\n",
    "        # noise variance 1e-6: beta ~ Gamma(beta | shape, rate)\n",
    "        ## Nick shape = 10 rate = 10*C0*args*.t**k0 = 10*0.2*0.005^3 = 2.5e-7\n",
    "        self.beta_prior_shape = 2.\n",
    "        self.beta_prior_rate = pde_noise\n",
    "\n",
    "        # for the equation loglikelihood\n",
    "        self.var_eq = 1e-4\n",
    "        self.lamda = np.ones((1,))  # TODO: dove lo uso? posso toglierlo? dopo aver aggiunto param2loss\n",
    "\n",
    "        self.architecture_nn_act_times = []\n",
    "        self.architecture_nn_cond_velocity = []\n",
    "\n",
    "        # build \"self.num_neural_networks\" instances for both act_times and cond_velocity\n",
    "        instances_nn_act_times = []\n",
    "        instances_nn_cond_velocity = []\n",
    "\n",
    "        for i in range(self.num_neural_networks):\n",
    "            new_instance_time = Net(nFeature, nLayer, nNeuron, 1)\n",
    "            new_instance_vel = Net(nFeature, nLayer, nNeuron, nOutput_vel)\n",
    "                # save the dimension of one network\n",
    "            if(i==0):\n",
    "                self.architecture_nn_act_times = new_instance_time.get_dimensions()\n",
    "                self.architecture_nn_cond_velocity = new_instance_vel.get_dimensions()\n",
    "\n",
    "            # TODO: sistemare prior -> initialization (senza questo pezzo uso una classica inizial. di un Neural Network)\n",
    "            ### w_i ~ StudentT(w_i | mu=0, lambda=shape/rate, nu=2*shape)\n",
    "            # initialization_vector = tfd.StudentT(df = self.w_prior_shape, loc = 0.0,\n",
    "            #                  scale = self.w_prior_rate).sample(sample_shape=(new_instance.num_parameters(),))\n",
    "            # new_instance.update_weights(initialization_vector.numpy())\n",
    "\n",
    "            instances_nn_act_times.append(new_instance_time) # append the i-th NN\n",
    "            instances_nn_cond_velocity.append(new_instance_vel)\n",
    "\n",
    "        # store all the Neural Networks\n",
    "        self.nnets_times = instances_nn_act_times\n",
    "        self.nnets_vel = instances_nn_cond_velocity\n",
    "\n",
    "        # store the arg \"param2loss\", used in criterion()\n",
    "        self.param2loss = param2loss\n",
    "    ###############################################################################################################################\n",
    "\n",
    "\n",
    "    # get the neural networks of index=idx\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.nnets_times[idx], self.nnets_vel[idx])\n",
    "\n",
    "\n",
    "    # forward pass of all the n_samples neural networks\n",
    "    def forward(self, inputs):\n",
    "        output_times = []\n",
    "        output_vel = []\n",
    "        for i in range(self.n_samples):\n",
    "            output_times.append(self.nnets_times[i].features(inputs))\n",
    "            output_vel.append(self.nnets_vel[i].features(inputs))\n",
    "        output_times = tf.stack(output_times)\n",
    "        output_vel = tf.stack(output_vel)\n",
    "        return output_times, output_vel\n",
    "\n",
    "    def get_trainable_weights(self):\n",
    "        weights_times = []\n",
    "        weights_vel = []\n",
    "        for i in range(self.num_neural_networks):\n",
    "            weights_times.append(self.nnets_times[i].features.trainable_weights)\n",
    "            weights_vel.append(self.nnets_vel[i].features.trainable_weights)\n",
    "\n",
    "        return weights_times, weights_vel\n",
    "\n",
    "    # ### TODO: use \"save\" instead of \"save_weights\"!\n",
    "    # # save all the neural networks\n",
    "    # def save_networks(self, path):\n",
    "    #     for i in range(self.n_samples):\n",
    "    #         save_path = os.path.join(path, \"weights_\"+str(i)+\".h5\")\n",
    "    #         self.nnets[i].features.save_weights(save_path)\n",
    "    #\n",
    "    # # load all the neural networks\n",
    "    # def load_networks(self, path):\n",
    "    #     for i in range(self.n_samples):\n",
    "    #         load_path = os.path.join(path,\"weights_\"+str(i)+\".h5\")\n",
    "    #         self.nnets[i].features.load_weights(load_path)\n",
    "\n",
    "    # compute the log joint probability = loglikelihood + log_prior_w + log_prior_log_beta\n",
    "    @tf.function    # decorator @tf.function to speed up the computation\n",
    "    def log_joint(self, index, output, target):\n",
    "        \"\"\"Log joint probability or unnormalized posterior for single model\n",
    "        instance. Ignoring constant terms for efficiency.\n",
    "\n",
    "        Args:\n",
    "            index (int): model index, 0, 1, ..., \"n_samples\"\n",
    "            output (Tensor): y_pred from our model\n",
    "            target (Tensor): y true\n",
    "        Returns:\n",
    "            Log joint probability (zero-dim tensor)\n",
    "        \"\"\"\n",
    "        # Normal(target | output, 1 / beta * I)\n",
    "        #print('output.size = ',output.size(0))\n",
    "        ntrain = output.shape[0]\n",
    "\n",
    "\n",
    "        output = tf.dtypes.cast(output, dtype=tf.float64)\n",
    "        ### train on data for JUST at -> target[:,0]\n",
    "        ### TODO: mettere un coeff n_exact/n_coll davanti?\n",
    "        log_likelihood = -0.50*tf.math.exp(self.nnets[index].log_beta)*tf.reduce_sum((target[:,0] - output[:,0])**2) \\\n",
    "                            + 0.50 * (tf.size(target[:,0], out_type = tf.dtypes.float64)) * self.nnets[index].log_beta\n",
    "\n",
    "\n",
    "        # log prob of prior of weights, i.e. log prob of studentT\n",
    "        log_prob_prior_w = 0.\n",
    "        for param in self.nnets[index].features.trainable_weights:\n",
    "            log_prob_prior_w += (tf.reduce_sum( tf.math.log1p( 0.5 / self.w_prior_rate * (param**2) ) ) )\n",
    "        log_prob_prior_w *= -(self.w_prior_shape + 0.5)\n",
    "        #print(log_prob_prior_w)\n",
    "\n",
    "        # log prob of prior of log noise-precision (NOT noise precision)\n",
    "        # noise prior (beta_shape - 1)*log_beta\n",
    "        log_prob_prior_log_beta = ((self.beta_prior_shape-1) * self.nnets[index].log_beta-  self.beta_prior_rate * (tf.math.exp(self.nnets[index].log_beta)) )\n",
    "        #print(log_prob_prior_log_beta)\n",
    "        log_likelihood=tf.dtypes.cast(log_likelihood, dtype=tf.float64)\n",
    "        log_prob_prior_w=tf.dtypes.cast(log_prob_prior_w, dtype=tf.float64)\n",
    "        log_prob_prior_log_beta=tf.dtypes.cast(log_prob_prior_log_beta, dtype=tf.float64)\n",
    "        return (log_likelihood + log_prob_prior_w + log_prob_prior_log_beta)\n",
    "\n",
    "    # compute the derivative of at and v wrt x and y\n",
    "    @tf.function    # decorator @tf.function to speed up the computation\n",
    "    def _gradients(self,index,x,y,z):\n",
    "        with tf.GradientTape(persistent = True) as t1:\n",
    "            t1.watch(x)\n",
    "            t1.watch(y)\n",
    "            if(self.n_input == 3):\n",
    "                t1.watch(z)\n",
    "                net_in = tf.concat((x,y,z),1)\n",
    "            else:\n",
    "                net_in = tf.concat((x,y),1) #input for NN\n",
    "            output = self.nnets[index].features(net_in) #output after forward(input)\n",
    "            at = output[:,0]    #activation time\n",
    "            v = output[:,1]     #local speed\n",
    "\n",
    "\n",
    "        at_x = t1.gradient(at, x)\n",
    "        v_x = t1.gradient(v, x)\n",
    "        at_y = t1.gradient(at, y)\n",
    "        v_y = t1.gradient(v, y)\n",
    "        if(self.n_input == 3):\n",
    "            at_z = t1.gradient(at, z)\n",
    "            v_z = t1.gradient(v, z)\n",
    "        else:\n",
    "            at_z = None\n",
    "            v_z = None\n",
    "        del t1\n",
    "        return at_x,v_x,at_y,v_y,at_z,v_z,v,output.shape[0]\n",
    "\n",
    "    # compute the loss and logloss of Physics Constrain (PDE constraint)\n",
    "    def criterion(self,index,x,y,z,ntrain_exact):\n",
    "        \"\"\"\n",
    "        Compute the loss and logloss of PDE constraint:\n",
    "        here we are using the Eikonal eq  =>  || Grad(at) || = 1/v\n",
    "        (we can rewrite it in a residual form as:\n",
    "        res = ||Grad(at)||*v - 1 = 0 )\n",
    "        We are also penalizing big gradients for the conduction_velocity\n",
    "\n",
    "        Args:\n",
    "            index (int): model index, 0, 1, ..., \"n_samples\"\n",
    "            x,y: batch of collocation data\n",
    "            ntrain_exact: number of exact data we have\n",
    "        Returns:\n",
    "            logloss1+logloss2: total log loss of pde\n",
    "            loss_1: loss of first constraint (Eikonal)\n",
    "            loss_2: loss of second constraint (No High gradients in velocity)\n",
    "        \"\"\"\n",
    "        # compute the derivatives\n",
    "        at_x,v_x, at_y,v_y, at_z,v_z, v,output_size = self._gradients(index,x,y,z)\n",
    "\n",
    "        # Eikonal eq constraint: res = ||Grad(at)||*v - 1 = 0\n",
    "        v = tf.dtypes.cast(v, tf.float64)\n",
    "        v = tf.reshape(v, shape=(v.shape[0],1))\n",
    "\n",
    "        if(self.n_input == 3):\n",
    "            # pde loss -> for collocation points\n",
    "            loss_1 = tf.math.multiply( (tf.math.sqrt(tf.math.square(at_x) + tf.math.square(at_y)+ tf.math.square(at_z))) , v) -1\n",
    "            # penalizing high gradients of V -> param2loss * Grad(v) small enough\n",
    "            loss_2 = self.param2loss*tf.math.sqrt(tf.math.square(v_x) + tf.math.square(v_y) + tf.math.square(v_z))\n",
    "        else:\n",
    "            # pde loss -> for collocation points\n",
    "            loss_1 = tf.math.multiply( (tf.math.sqrt(tf.math.square(at_x) + tf.math.square(at_y))) , v) -1\n",
    "            # penalizing high gradients of V -> param2loss * Grad(v) small enough\n",
    "            loss_2 = self.param2loss*tf.math.sqrt(tf.math.square(v_x) + tf.math.square(v_y))\n",
    "\n",
    "        # TODO: sistemare il fattore ntrain_exact/output_size... è giusto? serve?\n",
    "        # log loss for a Gaussian\n",
    "        logloss1 = ntrain_exact / output_size * (\n",
    "                            - 0.5 * self.nnets[index].beta_eq\n",
    "                            * tf.reduce_sum((loss_1 - tf.zeros_like(loss_1))**2)\n",
    "                            )\n",
    "        # log loss for a Gaussian\n",
    "        logloss2 = ntrain_exact / output_size * (\n",
    "                            - 0.5 * self.nnets[index].beta_eq\n",
    "                            * tf.reduce_sum((loss_2 - tf.zeros_like(loss_2))**2)\n",
    "                            )\n",
    "\n",
    "        return (logloss1+logloss2), loss_1, loss_2\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Prediction at data_test = inputs.\n",
    "        Args:\n",
    "            input (Tensor): [N, 2], test input\n",
    "        Returns:\n",
    "            y (Tensor): shape=[n_samples,2] prediction of (at,v) for all n_samples neural networks\n",
    "        \"\"\"\n",
    "        y = self.forward(inputs)\n",
    "        return y\n",
    "\n",
    "    def mean_and_variances(self, inputs):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = BayesNN(5, 0.01, 2, 3, 20, 1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tim,vel = bnn.get_trainable_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
